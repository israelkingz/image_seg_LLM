# -*- coding: utf-8 -*-
"""LLM_code_example_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A8dQ5gE9Oo6AsOHwdSIcWNvUnaLG-lOT

# 1.0 Introduction
We all love movies and we spend a lot of time watching movies. But one thing we spend more time doing is choosing the perfect movie to watch. One way to avoid picking a bad movie is reading reviews which can be tasking at time in a case whereby you have to read a lot of reviews to know if a movie is bad or not.  The total spent on reading reviews has been reduced by the rise of machine learning especially sentimental analysis in the natural language processing branch of machine learning.
 Every review posted under a movie is classified into positive or negative review the total count of each class can be viewed under the movie. This promptly reduces the amount of time spent to choose which movie to watch and significantly increase the amount of time spent streaming. Thus, increase movie and streaming company’s revenue.
In this task we will be training a Bert Large language model on IMDB movie reviews dataset. The dataset has movie reviews over the years and each review has been classified as either negative or positive reviews.
"""

# import clear out to clear output during library installation
from IPython.display import clear_output

# install transformers and keras tuner
!pip install transformers tensorflow
!pip install keras-tuner
clear_output()

# import libraries
import pandas as pd
import numpy as np
import transformers
import tensorflow as tf
from kerastuner.tuners import RandomSearch
import ipywidgets as widgets
from IPython.display import display, clear_output
import matplotlib.pyplot as plt
import tensorflow_datasets as tfds
from transformers import BertTokenizer, TFBertForSequenceClassification
from sklearn.model_selection import train_test_split

# pandas settings to view all rows and columns
pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)

"""# 2.0 Methodology
The BERT large language (Bidirectional Encoder Representations from Transformers) architecture will be the base model use in this task to classify reviews into positive and negative. BERT was trained on Wikipedia and Google’s BooksCorpus which have approximately 3 billion words combined. These large datasets contributed to BERT’s deep knowledge not only of the English language but also of our world (B.Muller 2022).
An attention seeking mechanism is used by transformer models to understanding relationship between words. This mechanism has made transformer based model very successful at deep learning task especially natural language processing (NLP). The Bert for sequence classification model used in this task uses a multi-head attention where each head focus on a specific pattern in the data. The model has a Bert base model and an additional linear layer on top for sequences classification. This model is designed for case whereby sequences of input are required to be classified into predefined classes in this case Positive and negative.
The model output a logits where activation functions such as sigmoid or softmax is applied for either a binary classification or non-binary classification respectively to get the predicted class. The model is compiled using it the bert-base-uncased weight with 110 million parameters. The “uncased” means the model converts all letters in a sentence to a lower case before tokenizing i.e. there is no difference between the word Turkey and turkey, Polish and polish, Lima and lima in the model.

"""

# download imdb data from tensorflow dataset library
(train_dataset, test_dataset), dataset_info = tfds.load('imdb_reviews', split=['train[:50%]', 'test[50:60%]'], shuffle_files=True, with_info=True, as_supervised=True)

# Convert the dataset to pandas dataframe
train_df = tfds.as_dataframe(train_dataset, dataset_info)

# split train set to train and val
train_df, val_df = train_test_split(train_df, random_state=123, test_size=0.1)
test_df = tfds.as_dataframe(test_dataset, dataset_info)

# view sample of train_df
train_df.head(10)

# check for null data
train_df.info()

# take acopy of train_df for eda
df = train_df.copy()
# count the label class into dictionary
label_count = df['label'].replace([0,1], ['Negative', 'Positive']).value_counts().to_dict()

# plot bar chart for label class
plt.style.use("dark_background")# set background to dark
plt.figure(figsize=(5, 8)) # set figure size
plt.bar(label_count.keys(), label_count.values()) # plot bar chart using label_count ddict
plt.xlabel('classes') # set xxaxis label
plt.ylabel('Number of labels in class') # set yaxis label
plt.title('Label distribution')
plt.show() # show plot

# check example of positive reviews
df_positive = df[df['label'] == 1]
print('Positive Review')

# iterate through rows to decode text and print out
for _, row in df_positive.sample(5).iterrows():
    print(row['text'].decode('utf-8'))
    print(' ')

# check example of negative reviews
df_negative = df[df['label'] == 0]
print('Negative Reviews')

# iterate through rows to decode text and print out
for _, row in df_negative.sample(5).iterrows():
    print(row['text'].decode('utf-8'))
    print(' ')

# Load the BERT tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# function that tokenize the dataframe and converts it into tf dataset
def preprocess(data):
    """
    function that tokenize the dataframe and converts it into tf dataset

    Args
    data(dataframe): takes in a pandas dataframe

    Returns:
    tf.data.dataset
    """
    texts = data['text'].tolist() # convert text to list
    texts = [s.decode('utf-8') for s in texts] # decode text into list
    labels = data['label'].tolist() # get labels

    encodings = tokenizer(texts, truncation=True, padding=True, max_length=512)

    dataset = tf.data.Dataset.from_tensor_slices((
        dict(encodings),
        labels
    ))
    return dataset

# apply preprocess function to train_df
train_dataset = preprocess(train_df)

# apply preprocess function to val_df
val_dataset = preprocess(val_df)

# appply preprocess function to test_df
test_dataset = preprocess(test_df)

"""# 3.0 Training and Fine Tuning
The train dataset was 50% of the total IMDB review dataset from the tensorflow dataset which was split into 9:1 where 10% of the train dataset was used for validation. The test dataset was a 10% set of the total IMDB review from the tensorflow dataset. The 2 classes in the dates are balanced and the bert-based-uncase tokenizer is used to convert the text in the dataset to lower cases before it can be used for training.
The keras-tunner library was used for the hyper parameter tuning i.e to select the most optimal parameter for training. A list of learning rate was used to check the most appropriate learning rate using accuracy as the metric.

After 3 trials with 2 epochs each the model with best performance measured by the accuracy metric is selected for further training. The best model has 93.28% validation accuracy and was trained for 5 epochs to improve on the overall accuracy. It takes around 7 minutes to complete an epoch using the V100 gpu on colab.
After further training of the best model the model performs impressively well on test dataset with an accuracy of 92.16%.

"""

# function to use keras tunner for optimal parameter search
def build_model(hp):
    """
    Args
    hp: list of learning rate

    returns
    tf.keras.Model: compiled model
    """
    # load bert model set num_label = 2
    model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

    #compile model with metrics, losses and list of learning rate
    model.compile(
        optimizer=tf.keras.optimizers.Adam(
            hp.Choice('learning_rate', [1e-5, 2e-5, 5e-5])
        ),
        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')]
    )
    return model

# intiate parameter search with function build model
tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=5,
    executions_per_trial=1
)

# initiate tunner search for optimal parameter
tuner.search(train_dataset.batch(14), validation_data=val_dataset.batch(14), epochs=2)

# view result summary
tuner.results_summary()

# get best model for longer training
best_model = tuner.get_best_models()[0]

# Fine-tuning the model
history = best_model.fit(train_dataset.batch(14), epochs=5, validation_data=val_dataset.batch(14) )

# Evaluate the model on the test data
best_model.evaluate(test_dataset.batch(4))

# evaluate model
loss, accuracy = best_model.evaluate(test_dataset.batch(4))
print(f'Final accuracy on the test dataset = {accuracy * 100:.2f}') #ptint accuracy

# save the trained model to google drive
best_model.save('/content/drive/MyDrive/llms/llms_best_model')

# get loss from training history
loss = history.history['loss']

#get accuracy for train history
accuracy = history.history['accuracy']

# get val_loss from training history
val_loss = history.history['val_loss']

# get val_accuracy from training history
val_accuracy = history.history['val_accuracy']

# get epochs from training history
epochs_range = range(len(loss))

#plot result
plt.style.use("dark_background") # set background to black
plt.figure(figsize=(10,5)) # set figure size
plt.suptitle("Losses and Metrics Plot", fontsize=15)
plt.subplot(1, 2, 1)
plt.plot(epochs_range, loss, label='Training loss') #plot
plt.plot(epochs_range, val_loss, label='val loss')# plot
plt.xlabel('number of epochs') #set x axis label for plot 1
plt.ylabel('loss') #set y axis label for plot 1
plt.legend(loc='upper right') # set legend
plt.title('Training and Validation loss') # set title
plt.subplot(1, 2, 2)
plt.plot(epochs_range, accuracy, label='Training accuracy') # plot
plt.plot(epochs_range, val_accuracy, label='val accuracy') # plot
plt.xlabel('number of epochs') #set x axis label for plot 2
plt.ylabel('accuracy') #set y axis label for plot 2
plt.legend(loc='lower right') # set legend
plt.title('Training and Validation accuracy') # set title for plot
plt.show() # show figure

"""# 4.0 Deployment
After the training of the model was completed the model was saved to a directory on google drive for easily access during deployment. The model was deployed using  ipywidgets library through UI interactive text box. Once a text is submitted through the text box with predict button the text is tokenized using the bert-base-uncase tokenizer. The tokened input is then passed to the trained model for prediction. The model’s output is in are in logits form that requires an activation function or further processing to get the predicted class. In this case we have implemented a np.argmax that selected the class with highest probability. The probability that the sentence also belong to a certain class is return.
"""

# load saved trained model
model = tf.keras.models.load_model("/content/drive/MyDrive/llms/llms_best_model")

# initiate tokenizer
tokenizer.from_pretrained('bert-base-uncased')

# Define function that predict the class label
def predict(text):
    """
    function that predict the class label for a text

    Args:
    text: a movie review that needs to be classified

    returns:
    predicted_class and predicted probability
    """
    # Tokenize input text
    inputs = tokenizer(text, return_tensors="tf", truncation=True, padding=True, max_length=512)

    # Model prediction
    prediction = model(inputs)
    logits = prediction['logits'].numpy()

    # Convert logits to probabilities
    probabilities = tf.nn.softmax(logits, axis=1).numpy()[0]

    # Get the predicted class and its probability
    predicted_class = np.argmax(probabilities)
    predicted_probability = probabilities[predicted_class]

    return predicted_class, predicted_probability


# Define a function `on_button_click` that is called when a button is pressed.
def on_button_click(button):
    # Clear the previous output from the display.
    clear_output(wait=True)

    # Call the `predict` function
    # The `predict` function returns two values: a prediction (either 1 or 0) and a probability.
    prediction, probability = predict(text_area.value)

    # If the prediction is 1, then print that the input text is predicted as positive.
    if prediction == 1:
        print(f"Prediction: Positive with probability {probability:.4f}%")
    # Otherwise, print that the input text is predicted as negative.
    else:
        print(f"Prediction: Negative with probability {probability:.4f}")

    # Call the `display_ui` function to display the text area and button again after showing the prediction.
    display_ui()

# Define a function `display_ui` that displays the `text_area` and `button` widgets.
def display_ui():
    display(text_area, button)

# Create a `text_area` widget using the `widgets.Textarea` method. This widget allows the user to input text.
text_area = widgets.Textarea(
    value='',  # Initial value of the text area is empty.
    placeholder='Type something',  # Placeholder text to display when the text area is empty.
    description='Input Text:',  # Label for the text area.
    disabled=False,  # The text area is enabled (i.e., it's editable).
    layout=widgets.Layout(height='100px', width='80%')  # Design specifications for the text area.
)

# Create a button widget using the `widgets.Button` method.
button = widgets.Button(description="Predict")

# Assign the `on_button_click` function to be called when the button is clicked.
button.on_click(on_button_click)

# Initially display the user interface
display_ui()

"""# 5.0 Conclusion
Although the model performance in making prediction on the dataset is impressive a lot of computational resources is still required to fine tune the pre trained Bert model for our task. For instance it takes around 7 minute to complete and epoch using a v100 gpu. Such computing power is not generally available on a regular pc. This makes fine tuning a Bert model for a natural language processing task resources and time intensive

"""