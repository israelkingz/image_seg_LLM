# -*- coding: utf-8 -*-
"""image-segmentation (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JNfWW3JfDnbNm4RnAUFWrgeJ6IYcS7WV
"""

!pip install pycocotools
!pip uninstall -y tensorflow-io
!pip install tensorflow-io

"""### Importing Required LibrariesÂ¶"""

# Commented out IPython magic to ensure Python compatibility.
import warnings
warnings.filterwarnings(action="ignore")

import os
import pylab
import array
import numpy as np
import skimage.io as io
from random import shuffle
from pycocotools.coco import COCO
from pycocotools import mask as maskUtils

from tensorflow import keras
from collections import defaultdict

from scipy.ndimage import binary_erosion, binary_dilation
from scipy.ndimage.filters import gaussian_filter

import matplotlib
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.patches as patches
plt.style.use("ggplot")
# %matplotlib inline

ANNOTATION_FILE_VAL = '/kaggle/input/rm-segmentation-assignment-dataset/validation-300/labels.json'
ANNOTATION_FILE_TRAIN = '/kaggle/input/rm-segmentation-assignment-dataset/train-300/labels.json'

coco_train = COCO(ANNOTATION_FILE_TRAIN)
coco_val = COCO(ANNOTATION_FILE_VAL)

num_train_images = len(coco_train.getImgIds())
num_val_images = len(coco_val.getImgIds())

print("Number of images in training set:", num_train_images)
print("Number of images in validation set:", num_val_images)

coco_train.getCatIds(['person', 'cake', 'dog', 'car'])

# Category IDs.
cat_ids = coco_train.getCatIds()
print(f"Number of Unique Categories: {len(cat_ids)}")
print("Category IDs:")
print(cat_ids)  # The IDs are not necessarily consecutive.

# All categories.
cats = coco_train.loadCats(cat_ids)
cat_names = [cat["name"] for cat in cats]
print("Categories Names:")
print(cat_names)

# Category ID -> Category Name.
query_id = cat_ids[15]
query_annotation = coco_train.loadCats([query_id])[0]
query_name = query_annotation["name"]
query_supercategory = query_annotation["supercategory"]
print("Category ID -> Category Name:")
print(
    f"Category ID: {query_id}, Category Name: {query_name}, Supercategory: {query_supercategory}"
)

# Category Name -> Category ID.
query_name = cat_names[16]
query_id = coco_train.getCatIds(catNms=[query_name])[0]
print("Category Name -> ID:")
print(f"Category Name: {query_name}, Category ID: {query_id}")

# Get the ID of all the images containing the object of the category.
img_ids = coco_train.getImgIds(catIds=[query_id])
print(f"Number of Images Containing {query_name}: {len(img_ids)}")

# Pick one image.
img_id = img_ids[2]
img_info = coco_train.loadImgs([img_id])[0]
img_file_name = img_info["file_name"]
img_url = img_info["coco_url"]
print(
    f"Image ID: {img_id}, File Name: {img_file_name}, Image URL: {img_url}"
)

# Get all the annotations for the specified image.
ann_ids = coco_train.getAnnIds(imgIds=[img_id], iscrowd=None)
anns = coco_train.loadAnns(ann_ids)
print(f"Annotations for Image ID {img_id}:")
print(anns)

"""### Displaying Image with Annotations"""

# Use URL to load image.
data_path = "/kaggle/input/rm-segmentation-assignment-dataset/train-300/data"
# Load image from dataset
im = plt.imread(os.path.join(data_path,coco_train.loadImgs(img_id)[0]['file_name']))
# Save image and its labeled version.
plt.axis("off")
plt.imshow(np.asarray(im))
plt.savefig(f"{img_id}.jpg", bbox_inches="tight", pad_inches=0)
# Plot segmentation and bounding box.
coco_train.showAnns(anns, draw_bbox=True)
plt.savefig(f"{img_id}_annotated.jpg", bbox_inches="tight", pad_inches=0)
plt.show()

"""### Visualizing Category Distribution in the COCO Dataset"""

included_categories = ['person', 'cake', 'dog', 'car']

# Filter categories based on names
catIDs = coco_train.getCatIds(catNms=included_categories)
cats = coco_train.loadCats(catIDs)

# Get category names
category_names = [cat['name'].title() for cat in cats]

# Get category counts
category_counts = [coco_train.getImgIds(catIds=[cat['id']]) for cat in cats]
category_counts = [len(img_ids) for img_ids in category_counts]

# Create a color palette for the plot
colors = sns.color_palette('viridis', len(category_names))

# Create a horizontal bar plot to visualize the category counts
plt.figure(figsize=(15, 7))
barplot = sns.barplot(y=category_counts, x=category_names, palette=colors)

for p in barplot.patches:
    h = p.get_height()
    barplot.text(p.get_x()+(p.get_width()/2), h, f"{h}", ha="center", va="bottom")

barplot.set_xlabel('Category', fontsize=15)
barplot.set_ylabel('Count', fontsize=15)
barplot.set_title('Category Distribution in Person, Cake, Dog, Car in COCO Train Dataset', fontsize=20)

# Rotate x-axis labels for better readability
barplot.set_xticklabels(labels=barplot.get_xticklabels(), rotation=45, ha='right')

plt.tight_layout()
plt.show()

"""### Visualizing Category Distribution as a Pie Chart"""

included_categories = ['person', 'cake', 'dog', 'car']

# Filter categories based on names
catIDs = coco_train.getCatIds(catNms=included_categories)
cats = coco_train.loadCats(catIDs)

# Get category names
category_names = [cat['name'].title() for cat in cats]

# Get category counts
category_counts = [coco_train.getImgIds(catIds=[cat['id']]) for cat in cats]
category_counts = [len(img_ids) for img_ids in category_counts]

# Calculate percentages
total_images = len(coco_train.getImgIds())
percentages = [(count / total_images) * 100 for count in category_counts]

# Create a color palette for the plot
colors = plt.cm.tab10(range(len(category_names)))  # Using the 'tab10' colormap for more colors

# Create a pie chart to visualize percentages
plt.figure(figsize=(8, 8))
plt.pie(percentages, labels=category_names, autopct='%1.1f%%', colors=colors, startangle=140)
plt.title('Category Distribution in Person, Cake, Dog, Car Train Dataset', fontsize=20)
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

plt.tight_layout()
plt.show()

"""### Displaying Filtered Images with Annotations"""

filter_classes = ['person', 'cake', 'dog', 'car']

# Fetch class IDs only corresponding to the filterClasses
catIds = coco_train.getCatIds(catNms=filter_classes)

# Get all images containing the above Category IDs
imgIds = coco_train.getImgIds(imgIds=catIds)

imgIds

# Load a random image from the filtered list
if len(imgIds) > 0:
    for image_id in imgIds:
        image_info = coco_train.loadImgs(image_id)

        if image_info is not None and len(image_info) > 0:
            image_info = image_info[0]
            image_path = os.path.join(data_path , image_info['file_name'])

            # Load the annotations for the image
            annotation_ids = coco_train.getAnnIds(imgIds=image_id)
            annotations = coco_train.loadAnns(annotation_ids)

            # Get category names and assign colors for annotations
            category_names = [coco_train.loadCats(ann['category_id'])[0]['name'].capitalize() for ann in annotations]
            category_colors = list(matplotlib.colors.TABLEAU_COLORS.values())

            # Load the image and plot it
            image = plt.imread(image_path)
            plt.imshow(image)
            plt.axis('off')
            plt.title('Annotations for Image ID: {}'.format(image_id))
            plt.tight_layout()
            plt.savefig(f'Img_{image_id}.png', dpi=350)
            plt.show()

            plt.imshow(image)
            plt.axis('off')

            # Display bounding boxes and segmented colors for each annotation
            for ann, color in zip(annotations, category_colors):
                bbox = ann['bbox']
                segmentation = ann['segmentation']

                # Display bounding box
                rect = patches.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3], linewidth=1,
                                         edgecolor=color, facecolor='none')
                plt.gca().add_patch(rect)

                # Display segmentation masks with assigned colors
                for seg in segmentation:
                    poly = np.array(seg).reshape((len(seg) // 2, 2))
                    plt.fill(poly[:, 0], poly[:, 1], color=color, alpha=0.6)

            # Create a legend with category names and colors
            legend_patches = [patches.Patch(color=color, label=name) for color, name in zip(category_colors, category_names)]
            plt.legend(handles=legend_patches, loc="lower center", ncol=4, bbox_to_anchor=(0.5, -0.2), fontsize='small')

            # Show the image with legend
            plt.title('Annotations for Image ID: {}'.format(image_id))
            plt.tight_layout()
            plt.savefig(f'annImg_{image_id}.png', dpi=350)
            plt.show()
else:
    print("No images found for the desired classes.")

"""### Generating Masks for Object Segmentation"""

image_ids = coco_train.getImgIds()
image_id = image_ids[2]
annotations = coco_train.loadAnns(coco_train.getAnnIds(imgIds=image_id))
annotations

image_info = coco_train.loadImgs(image_id)[0]
image_path = os.path.join(data_path, image_info['file_name'])

# Load the main image
main_image = plt.imread(image_path)

# Create a new figure for displaying the main image
plt.figure(figsize=(10, 10))
plt.imshow(main_image)
plt.axis('off')
plt.title('Main Image')

# Save the figures
plt.savefig('main_image.png', dpi=300)

# Show the plots
plt.show()

"""### Generating Binary Masks"""

image_info = coco_train.loadImgs(image_id)[0]
height, width = image_info['height'], image_info['width']

# Create an empty binary mask with the same dimensions as the image
binary_mask = np.zeros((height, width), dtype=np.uint8)

# Iterate through the annotations and draw the binary masks
for annotation in annotations:
    segmentation = annotation['segmentation']
    mask = coco_train.annToMask(annotation)

    # Add the mask to the binary mask
    binary_mask += mask

# Display the binary mask
plt.figure(figsize=(10,10))
plt.imshow(binary_mask, cmap='gray')
plt.axis('off')
plt.title('Binary Mask')
plt.savefig('binary_mask.png', dpi=300)
plt.show()

"""### Generating RGB Mask"""

image_info = coco_train.loadImgs(image_id)[0]
height, width = image_info['height'], image_info['width']

# Create an empty RGB mask with the same dimensions as the image
rgb_mask = np.zeros((height, width, 3), dtype=np.uint8)

# Define a color map for different object classes
color_map = {cat['id']: (np.random.randint(0, 256), np.random.randint(0, 256), np.random.randint(0, 256))
             for cat in coco_train.loadCats(catIDs)}

# Print the color map for debugging
print("Color Map:")
print(color_map)

# Iterate through the annotations and assign unique colors to each class/object
for annotation in annotations:
    category_id = annotation['category_id']

    # Print category IDs for debugging
    print("Category ID:", category_id)

    color = color_map.get(category_id)  # Use .get() to avoid KeyError

    if color is not None:
        # Draw the mask on the RGB mask
        mask = coco_train.annToMask(annotation)
        rgb_mask[mask == 1] = color
    else:
        print("Category ID {} not found in color map.".format(category_id))

# Display the RGB mask
plt.figure(figsize=(10,10))
plt.imshow(rgb_mask)
plt.axis('off')
plt.title('RGB Mask')
plt.savefig('rgb_mask.png', dpi=300)
plt.show()

"""### Generating Instance Segmentation Mask"""

image_info = coco_train.loadImgs(image_id)[0]
height, width = image_info['height'], image_info['width']

# Create an empty mask with the same dimensions as the image
instance_mask = np.zeros((height, width), dtype=np.uint8)

# Iterate through the annotations and draw the instance segmentation masks
for annotation in annotations:
    segmentation = annotation['segmentation']
    mask = coco_train.annToMask(annotation)
    category_id = annotation['category_id']

    # Assign a unique value to each instance mask
    instance_mask[mask == 1] = category_id

# Display the instance segmentation mask
plt.figure(figsize=(10,10))
plt.imshow(instance_mask, cmap='viridis')
plt.axis('off')
plt.title('Instance Segmentation Mask')
plt.savefig('instance_mask.png', dpi=300)
plt.show()

"""### Generating Object Detection Bounding Boxes"""

image_info = coco_train.loadImgs(image_id)[0]
height, width = image_info['height'], image_info['width']

# Create a new figure with the same dimensions as the image
fig, ax = plt.subplots(figsize=(10,10), dpi=100)

# Display the original image
ax.imshow(main_image)
ax.axis('off')
ax.set_title('Original Image')

# Draw bounding boxes on the original image
for annotation in annotations:
    bbox = annotation['bbox']
    category_id = annotation['category_id']
    category_name = coco_train.loadCats(category_id)[0]['name']

    # Convert COCO bounding box format (x, y, width, height) to matplotlib format (xmin, ymin, xmax, ymax)
    xmin, ymin, width, height = bbox
    xmax = xmin + width
    ymax = ymin + height

    # Draw the bounding box rectangle
    rect = patches.Rectangle((xmin, ymin), width, height, linewidth=1, edgecolor='red', facecolor='none')
    ax.add_patch(rect)

    # Add the category name as a label above the bounding box
    ax.text(xmin, ymin - 5, category_name, fontsize=8, color='red', weight='bold')

# Save the figure with adjusted dimensions
plt.savefig('bounding_boxes.png', bbox_inches='tight')

# Show the plot
plt.show()

"""### Post-Processing Techniques"""

# Apply erosion to the binary mask
eroded_mask = binary_erosion(binary_mask)

# Apply dilation to the binary mask
dilated_mask = binary_dilation(binary_mask)

# Apply Gaussian blur to the binary mask
smoothed_mask = gaussian_filter(binary_mask, sigma=.2)

# Display the post-processed masks
fig, axes = plt.subplots(3, 1, figsize=(12, 12))

axes[0].imshow(eroded_mask, cmap='gray')
axes[0].set_title('Eroded Mask')
axes[0].axis('off')

axes[1].imshow(dilated_mask, cmap='gray')
axes[1].set_title('Dilated Mask')
axes[1].axis('off')

axes[2].imshow(smoothed_mask, cmap='gray')
axes[2].set_title('Smoothed Mask')
axes[2].axis('off')

plt.tight_layout()
plt.savefig('post_processed_masks.png', dpi=300)
plt.show()

image_id = image_ids[2]

# Load the image
image_info = coco_train.loadImgs(image_id)[0]
image_path = os.path.join(data_path, image_info['file_name'])
image = plt.imread(image_path)

# Get the ground truth annotations for the image
annotation_ids = coco_train.getAnnIds(imgIds=image_id)
annotations = coco_train.loadAnns(annotation_ids)

# Create a blank image for overlaying the masks
overlay = image.copy()

# Iterate over the annotations and draw the masks on the overlay image
for annotation in annotations:
    # Get the segmentation mask
    mask = coco_train.annToMask(annotation)

    # Choose a random color for the mask
    color = np.random.randint(0, 256, size=(3,), dtype=np.uint8)

    # Apply the mask to the overlay image
    overlay[mask == 1] = color

# Create a figure and subplot for visualization
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))

# Plot the original image
ax1.imshow(image)
ax1.set_title('Original Image')
ax1.axis('off')

# Plot the image with overlay masks
ax2.imshow(overlay)
ax2.set_title('Masks Overlay')
ax2.axis('off')

# Adjust the spacing between subplots
plt.tight_layout()

# Save the visualization as an image file
plt.savefig('mask_visualization.png', dpi=300)

# Show the plot
plt.show()

"""### Data Generation"""

def convert_masks(split, num_classes=-1):
    annotation_file = f'/kaggle/input/rm-segmentation-assignment-dataset/{split}-300/labels.json'
    print(split, end="\n\n")
    coco = COCO(annotation_file)

    # get category ids
    category_ids = coco.getCatIds()
    print("Category IDs: ", category_ids)
    print("Categories: ", [cat["name"] for cat in coco.loadCats(ids=category_ids)])

    # get image ids
    image_ids = coco.getImgIds()
    print(f"Number of images: {len(image_ids)}")

    # Create a directory to store the generated masks
    if split == "train":
        directory_path = '/kaggle/working/train-300/masks'
    else:
        directory_path = '/kaggle/working/validation-300/masks'

    # Create the directory
    os.makedirs(directory_path, exist_ok=True)

    # Mapping categories to pixel values
    if split == "train":
        pixel_map = {15: 1, 16: 2, 25: 3, 41: 4}
    else:
        pixel_map = {14: 1, 15: 2, 24: 3, 41: 4}

    # Generate masks
    for i_img, img_id in enumerate(image_ids):
        img = coco.loadImgs(img_id)[0]
        annotation_ids = coco.getAnnIds(imgIds=img_id, catIds=category_ids, iscrowd=None)
        annotations = coco.loadAnns(annotation_ids)
        mask = np.zeros([img["height"], img["width"]], dtype="uint8")
        for ann in annotations:
            try:
                ann_mask = coco.annToMask(ann)
                category_id = ann["category_id"]
                pixel_value = pixel_map.get(category_id, 0)  # Default value is 0 if category_id not found
                mask[ann_mask > 0] = pixel_value
            except Exception as e:
                print(f"Error processing annotation: {ann}")
                print(f"Error message: {e}")

        file_path = "/kaggle/working/{}-300/masks/{}".format(split, img["file_name"][:-4])
        mask = np.expand_dims(mask, axis=-1)

        if i_img == 0:
            print(np.unique(mask), mask.shape, img["file_name"])

        np.save(file_path, mask)

    print(f"Successfully completed the {split} data.")

convert_masks("train", 4)

convert_masks("validation", 4)

import cv2
from PIL import Image

def preprocess_data(images_path, masks_path):
    # Get a list of unique filenames for images and masks
    image_files = set([os.path.splitext(filename)[0] for filename in os.listdir(images_path)])
    mask_files = set([os.path.splitext(filename)[0] for filename in os.listdir(masks_path)])

    # Find the filenames that exist in both sets
    matching_files = list(image_files.intersection(mask_files))

    # Load images and masks for the matching filenames
    images = [Image.open(os.path.join(images_path, f"{filename}.jpg")) for filename in matching_files]
    masks = [np.load(os.path.join(masks_path, f"{filename}.npy")) for filename in matching_files]

    processed_imgs = []
    processed_masks = []

    # Process each image and its corresponding mask
    for i, image_mask in enumerate(zip(images, masks)):
        image, mask = image_mask

        # Resize the image and mask
        image = image.resize((128, 128))
        resized_mask = cv2.resize(mask, (128, 128), interpolation=cv2.INTER_NEAREST)

        # Convert image to numpy array and normalize if it's in the correct shape
        preprocessed_image = np.array(image)
        if len(preprocessed_image.shape) == 3 and preprocessed_image.shape == (128, 128, 3):
            preprocessed_image = preprocessed_image / 255.0
        else:
            print(i, ":", matching_files[i])
            continue

        # Convert mask to numpy array
        preprocessed_mask = np.array(resized_mask, dtype=np.float32)

        processed_imgs.append(preprocessed_image)
        processed_masks.append(preprocessed_mask)

    processed_imgs = np.array(processed_imgs)
    processed_masks = np.array(processed_masks)
    processed_masks = np.expand_dims(processed_masks, axis=3)

    print(processed_imgs.shape, processed_masks.shape)
    return processed_imgs, processed_masks

images_path = '/kaggle/input/rm-segmentation-assignment-dataset/train-300/data'
masks_path = '/kaggle/working/train-300/masks'

train_images, train_masks = preprocess_data(images_path, masks_path)

# Get unique values and their counts from train_masks
unique_values, counts = np.unique(train_masks, return_counts=True)

# Print each unique value and its count
for value, count in zip(unique_values, counts):
    print(f"Unique Value: {value}, Count: {count}")

# Validation data paths
val_image_directory = "/kaggle/input/rm-segmentation-assignment-dataset/validation-300/data"
val_mask_directory = "/kaggle/working/validation-300/masks"

# Load validation images and masks
val_images, val_masks = preprocess_data(val_image_directory, val_mask_directory)

# Get unique values and their counts from val_masks
unique_values, counts = np.unique(val_masks, return_counts=True)

# Print each unique value and its count
for value, count in zip(unique_values, counts):
    print(f"Unique Value: {value}, Count: {count}")

# Importing necessary function for one-hot encoding
from tensorflow.keras.utils import to_categorical

# One-hot encode the training masks
train_masks_encoded = to_categorical(train_masks, num_classes=5)
print("Shape of one-hot encoded training masks:", train_masks_encoded.shape)
print("Unique values in one-hot encoded training masks:", np.unique(train_masks_encoded))

# One-hot encode the validation masks
val_masks_encoded = to_categorical(val_masks, num_classes=5)
print("Shape of one-hot encoded validation masks:", val_masks_encoded.shape)
print("Unique values in one-hot encoded validation masks:", np.unique(val_masks_encoded))

"""### Displaying Processed Images and Masks
Displays the initial 10 images along with their corresponding masks in the training dataset.
"""

# Iterate over the first 10 images
for i in range(10):
    # Load and display the mask image
    mask_img = train_masks[i]
    plt.subplot(1, 2, 1)
    plt.imshow(mask_img)
    plt.title('Mask Image')
    plt.axis('off')

    # Load and display the corresponding main image
    main_img = train_images[i]
    plt.subplot(1, 2, 2)
    plt.imshow(main_img)
    plt.title('Main Image')
    plt.axis('off')

    # Print the shapes of the images
    print('Mask Image Shape:', np.array(mask_img).shape)
    print('Main Image Shape:', np.array(main_img).shape)

    plt.tight_layout()
    plt.show()

# Extracting data for visualization
image_data = train_images[7]
one_hot_encoded_mask = train_masks_encoded[7]
normal_mask = train_masks[7]

# Printing shapes of the extracted data
print('Image Shape:', image_data.shape)
print('One-hot Encoded Mask Shape:', one_hot_encoded_mask.shape)
print('Normal Mask Shape:', normal_mask.shape)

# Visualizing the image and mask
plt.figure(figsize=(15, 6))

plt.subplot(2, 3, 1)
plt.title('Image')
plt.imshow(image_data)
plt.axis('off')

plt.subplot(2, 3, 2)
plt.title('Mask')
plt.imshow(normal_mask)
plt.axis('off')

# Displaying the visualization
plt.show()

"""## Building Unet Model"""

from keras.layers import Input, Conv2D, MaxPooling2D, Dropout, Conv2DTranspose, concatenate, BatchNormalization, Activation, MaxPool2D, Concatenate
from keras.models import Model
from keras.optimizers import Adam
from keras.callbacks import ModelCheckpoint, EarlyStopping
import keras.backend as K
import tensorflow as tf

def dice_metric(y_true, y_pred, smooth=1):
    y_true = tf.reshape(y_true, (-1,))
    y_pred = tf.reshape(y_pred, (-1,))
    intersection = tf.reduce_sum(y_true * y_pred)
    dice = (2.0 * intersection + smooth) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + smooth)
    return dice

def dice_loss(y_true, y_pred):
    return 1 - dice_metric(y_true, y_pred)

def iou_score(y_true, y_pred, smooth=1e-4):
    intersection = tf.reduce_sum(y_true * y_pred)
    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection
    return (intersection + smooth) / (union + smooth)

# Convolutional block
def conv_block(input, num_filters, dropout_rate):
    x = Conv2D(num_filters, 3, padding="same")(input)
    x = BatchNormalization()(x)
    x = Activation("relu")(x)
    x = Dropout(dropout_rate)(x)

    x = Conv2D(num_filters, 3, padding="same")(x)
    x = BatchNormalization()(x)
    x = Activation("relu")(x)

    return x

# Encoder block
def encoder_block(input, num_filters, dropout_rate):
    x = conv_block(input, num_filters, dropout_rate)
    p = MaxPool2D((2, 2))(x)
    return x, p

# Decoder block
def decoder_block(input, skip_features, num_filters, dropout_rate):
    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding="same")(input)
    x = Concatenate()([x, skip_features])
    x = conv_block(x, num_filters, dropout_rate)
    return x

# Building U-Net architecture
def build_unet(input_shape, n_classes, dropout_rate):
    inputs = Input(input_shape)

    s1, p1 = encoder_block(inputs, 64, dropout_rate)
    s2, p2 = encoder_block(p1, 128, dropout_rate)
    s3, p3 = encoder_block(p2, 256, dropout_rate)
    s4, p4 = encoder_block(p3, 512, dropout_rate)

    b1 = conv_block(p4, 1024, dropout_rate) # Bridge

    d1 = decoder_block(b1, s4, 512, dropout_rate)
    d2 = decoder_block(d1, s3, 256, dropout_rate)
    d3 = decoder_block(d2, s2, 128, dropout_rate)
    d4 = decoder_block(d3, s1, 64, dropout_rate)

    if n_classes == 1:  # Binary
      activation = 'sigmoid'
    else:
      activation = 'softmax'

    outputs = Conv2D(n_classes, 1, padding="same", activation=activation)(d4)
    print(activation)

    model = Model(inputs, outputs, name="U-Net")
    return model

# Define input shape, number of classes, and dropout rate
input_shape = (128, 128, 3)
num_classes = 5
dropout_rate = 0.3

# Build U-Net model
model = build_unet(input_shape, num_classes, dropout_rate)
model.summary()

# Define callbacks
callbacks = [
    ModelCheckpoint(filepath="unet1.keras", monitor="val_loss", verbose=1, save_best_only=True),
    EarlyStopping(patience=5, restore_best_weights=True)
]

# Compile the model
model.compile(
    optimizer=Adam(learning_rate=1e-4),
    loss='categorical_crossentropy',
    metrics=[iou_score, dice_metric]
)

# Training the model
history = model.fit(
    train_images, train_masks_encoded,
    batch_size=8,
    verbose=1,
    epochs=50,
    validation_data=(val_images, val_masks_encoded),
    callbacks=callbacks
)

# Get the dice metric values from history
best_dice = history.history["dice_metric"][17]
best_val_dice = history.history["val_dice_metric"][17]

# Print the information
print("The last model saved by ModelCheckpoint was at Epoch 18 with a dice metric of {:.4f} and a validation dice metric of {:.4f}".format(best_dice, best_val_dice))

# Plotting training and validation loss
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(loss) + 1)

plt.plot(epochs, loss, 'y', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Plotting training and validation dice score
dice = history.history['dice_metric']
val_dice = history.history['val_dice_metric']

plt.plot(epochs, dice, 'y', label='Training Dice Score')
plt.plot(epochs, val_dice, 'r', label='Validation Dice Score')
plt.title('Training and validation Dice Score')
plt.xlabel('Epochs')
plt.ylabel('Dice Score')
plt.legend()
plt.show()

# Plotting training and validation IOU score
IOU = history.history['iou_score']
val_IOU = history.history['val_iou_score']

plt.plot(epochs, IOU, 'y', label='Training IOU Score')
plt.plot(epochs, val_IOU, 'r', label='Validation IOU Score')
plt.title('Training and validation IOU Score')
plt.xlabel('Epochs')
plt.ylabel('IOU Score')
plt.legend()
plt.show()

from keras.models import load_model
loaded_model = load_model("/kaggle/working/unet1.keras", compile=False)

# Predicting masks for validation images
predictions = model.predict(val_images)

# Printing the shape of the prediction array
print("Shape of the predicted masks:", predictions.shape)

# Compute the argmax along the last axis of the predicted masks
y_pred_argmax = predictions.argmax(axis=-1)

# Print the shape and unique values of the resulting array
print("Shape of the predicted masks after argmax:", y_pred_argmax.shape)
print("Unique values in the predicted masks after argmax:", np.unique(y_pred_argmax))

# Using built-in Keras function to compute Mean IoU
from keras.metrics import MeanIoU

# Define the number of classes
num_classes = 5

# Initialize MeanIoU object
iou_keras = MeanIoU(num_classes=num_classes)

# Update the state with ground truth masks and predicted masks
iou_keras.update_state(val_masks[:, :, :, 0], y_pred_argmax)

# Print the computed Mean IoU
print("Mean IoU =", iou_keras.result().numpy())

import random

# Randomly select an image from the validation set
test_img_number = random.randint(0, len(val_images)-1)
print("Test Image Number:", test_img_number)

# Extract the selected image, its ground truth mask, and prepare it for prediction
test_img = val_images[test_img_number]
ground_truth = val_masks[test_img_number]
test_img_input = np.expand_dims(test_img, 0)

# Predict the mask for the selected image
prediction = model.predict(test_img_input)[0]
pred = np.expand_dims(prediction.argmax(axis=-1), axis=-1)

# Print shapes of the selected image, ground truth mask, and predicted mask
print("Image Shape:", test_img.shape)
print("Ground Truth Mask Shape:", ground_truth.shape)
print("Predicted Mask Shape:", pred.shape)

# Plot the original image, ground truth mask, and predicted mask
plt.figure(figsize=(12, 8))

# Original image
plt.subplot(1, 3, 1)
plt.title('Original Image')
plt.imshow(test_img)
plt.axis('off')

# Ground truth mask
plt.subplot(1, 3, 2)
plt.title('Ground Truth Mask')
plt.imshow(ground_truth)
plt.axis('off')

# Predicted mask
plt.subplot(1, 3, 3)
plt.title('Predicted Mask')
plt.imshow(pred)
plt.axis('off')

plt.show()

import random

def return_test_images(images_path):
    # Get the list of image filenames from the specified directory
    image_filenames = [filename for filename in os.listdir(images_path)]
    # Randomly select 4 image filenames
    test_image_filenames = random.choices(image_filenames, k=4)

    # Load and preprocess the selected test images
    test_images = [Image.open(os.path.join(images_path, filename)).resize((128, 128)) for filename in test_image_filenames]
    processed_test_images = []

    # Process each test image
    for image in test_images:
        preprocessed_image = np.array(image)
        # Normalize the image if it meets the required shape
        if len(preprocessed_image.shape) == 3 and preprocessed_image.shape == (128, 128, 3):
            preprocessed_image = preprocessed_image / 255.0
        else:
            print("Invalid image shape:", preprocessed_image.shape)
            continue
        processed_test_images.append(preprocessed_image)

    processed_test_images = np.array(processed_test_images)

    print("Processed Test Images Shape:", processed_test_images.shape)
    return processed_test_images

# Path to the directory containing test images
test_images_path = '/kaggle/input/rm-segmentation-assignment-dataset/test-30'
test_images = return_test_images(test_images_path)

for i, image in enumerate(test_images):
    # Expand the dimensions to make it suitable for model prediction
    image_input = np.expand_dims(image, 0)

    # Get the prediction from the model
    prediction = model.predict(image_input)[0]
    # Convert prediction to a binary mask
    predicted_mask = np.expand_dims(prediction.argmax(axis=-1), axis=-1)

    # Print the shapes of the original image and the predicted mask
    print("Original Image Shape:", image.shape)
    print("Predicted Mask Shape:", predicted_mask.shape)

    # Plotting
    plt.figure(figsize=(12, 8))

    # Plot the original image
    plt.subplot(1, 2, 1)
    plt.title('Original Image')
    plt.imshow(image)
    plt.axis('off')

    # Plot the predicted mask
    plt.subplot(1, 2, 2)
    plt.title('Predicted Mask')
    plt.imshow(predicted_mask)
    plt.axis('off')

    plt.tight_layout()
    plt.show()

import cv2
from PIL import Image

def preprocess_data(images_path, masks_path):
    # Get a list of unique filenames for images and masks
    image_files = set([os.path.splitext(filename)[0] for filename in os.listdir(images_path)])
    mask_files = set([os.path.splitext(filename)[0] for filename in os.listdir(masks_path)])

    # Find the filenames that exist in both sets
    matching_files = list(image_files.intersection(mask_files))

    # Load images and masks for the matching filenames
    images = [Image.open(os.path.join(images_path, f"{filename}.jpg")) for filename in matching_files]
    masks = [np.load(os.path.join(masks_path, f"{filename}.npy")) for filename in matching_files]

    processed_imgs = []
    processed_masks = []

    # Process each image and its corresponding mask
    for i, image_mask in enumerate(zip(images, masks)):
        image, mask = image_mask

        # Resize the image and mask
        image = image.resize((224, 224))
        resized_mask = cv2.resize(mask, (224, 224), interpolation=cv2.INTER_NEAREST)

        # Convert image to numpy array and normalize if it's in the correct shape
        preprocessed_image = np.array(image)
        if len(preprocessed_image.shape) == 3 and preprocessed_image.shape == (224, 224, 3):
            preprocessed_image = preprocessed_image / 255.0
        else:
            print(i, ":", matching_files[i])
            continue

        # Convert mask to numpy array
        preprocessed_mask = np.array(resized_mask, dtype=np.float32)

        processed_imgs.append(preprocessed_image)
        processed_masks.append(preprocessed_mask)

    processed_imgs = np.array(processed_imgs)
    processed_masks = np.array(processed_masks)
    processed_masks = np.expand_dims(processed_masks, axis=3)

    print(processed_imgs.shape, processed_masks.shape)
    return processed_imgs, processed_masks

images_path = '/kaggle/input/rm-segmentation-assignment-dataset/train-300/data'
masks_path = '/kaggle/working/train-300/masks'

train_images, train_masks = preprocess_data(images_path, masks_path)

# Validation data paths
val_image_directory = "/kaggle/input/rm-segmentation-assignment-dataset/validation-300/data"
val_mask_directory = "/kaggle/working/validation-300/masks"

# Load validation images and masks
val_images, val_masks = preprocess_data(val_image_directory, val_mask_directory)

# Importing necessary function for one-hot encoding
from tensorflow.keras.utils import to_categorical

# One-hot encode the training masks
train_masks_encoded = to_categorical(train_masks, num_classes=5)
print("Shape of one-hot encoded training masks:", train_masks_encoded.shape)
print("Unique values in one-hot encoded training masks:", np.unique(train_masks_encoded))

# One-hot encode the validation masks
val_masks_encoded = to_categorical(val_masks, num_classes=5)
print("Shape of one-hot encoded validation masks:", val_masks_encoded.shape)
print("Unique values in one-hot encoded validation masks:", np.unique(val_masks_encoded))

# Extracting data for visualization
image_data = train_images[7]
one_hot_encoded_mask = train_masks_encoded[7]
normal_mask = train_masks[7]

# Printing shapes of the extracted data
print('Image Shape:', image_data.shape)
print('One-hot Encoded Mask Shape:', one_hot_encoded_mask.shape)
print('Normal Mask Shape:', normal_mask.shape)

from keras.layers import Input, Conv2D, MaxPooling2D, Dropout, Conv2DTranspose, concatenate, BatchNormalization, Activation, MaxPool2D, Concatenate
from keras.models import Model
from keras.optimizers import Adam
from keras.callbacks import ModelCheckpoint, EarlyStopping
import keras.backend as K
import tensorflow as tf

def dice_metric(y_true, y_pred, smooth=1):
    y_true = tf.reshape(y_true, (-1,))
    y_pred = tf.reshape(y_pred, (-1,))
    intersection = tf.reduce_sum(y_true * y_pred)
    dice = (2.0 * intersection + smooth) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + smooth)
    return dice

def dice_loss(y_true, y_pred):
    return 1 - dice_metric(y_true, y_pred)

def iou_score(y_true, y_pred, smooth=1e-4):
    intersection = tf.reduce_sum(y_true * y_pred)
    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection
    return (intersection + smooth) / (union + smooth)

# Convolutional block
def conv_block(input, num_filters, dropout_rate):
    x = Conv2D(num_filters, 3, padding="same")(input)
    x = BatchNormalization()(x)
    x = Activation("relu")(x)
    x = Dropout(dropout_rate)(x)

    x = Conv2D(num_filters, 3, padding="same")(x)
    x = BatchNormalization()(x)
    x = Activation("relu")(x)

    return x

# Encoder block
def encoder_block(input, num_filters, dropout_rate):
    x = conv_block(input, num_filters, dropout_rate)
    p = MaxPool2D((2, 2))(x)
    return x, p

# Decoder block
def decoder_block(input, skip_features, num_filters, dropout_rate):
    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding="same")(input)
    x = Concatenate()([x, skip_features])
    x = conv_block(x, num_filters, dropout_rate)
    return x

# Building U-Net architecture
def build_unet(input_shape, n_classes, dropout_rate):
    inputs = Input(input_shape)

    s1, p1 = encoder_block(inputs, 64, dropout_rate)
    s2, p2 = encoder_block(p1, 128, dropout_rate)
    s3, p3 = encoder_block(p2, 256, dropout_rate)
    s4, p4 = encoder_block(p3, 512, dropout_rate)

    b1 = conv_block(p4, 1024, dropout_rate) # Bridge

    d1 = decoder_block(b1, s4, 512, dropout_rate)
    d2 = decoder_block(d1, s3, 256, dropout_rate)
    d3 = decoder_block(d2, s2, 128, dropout_rate)
    d4 = decoder_block(d3, s1, 64, dropout_rate)

    if n_classes == 1:  # Binary
        activation = 'sigmoid'
    else:
        activation = 'softmax'

    outputs = Conv2D(n_classes, 1, padding="same", activation=activation)(d4)
    print(activation)

    model = Model(inputs, outputs, name="U-Net")
    return model

# Define input shape, number of classes, and dropout rate
input_shape = (224, 224, 3)
num_classes = 5
dropout_rate = 0.3

# Build U-Net model
model = build_unet(input_shape, num_classes, dropout_rate)
model.summary()

# Define callbacks
callbacks = [
    ModelCheckpoint(filepath="unet1.keras", monitor="val_loss", verbose=1, save_best_only=True),
    EarlyStopping(patience=5, restore_best_weights=True)
]

# Compile the model
model.compile(
    optimizer=Adam(learning_rate=1e-4),
    loss='categorical_crossentropy',
    metrics=[iou_score, dice_metric]
)

# Training the model
history = model.fit(
    train_images, train_masks_encoded,
    batch_size=8,
    verbose=1,
    epochs=50,
    validation_data=(val_images, val_masks_encoded),
    callbacks=callbacks
)

# Plotting training and validation loss
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(loss) + 1)

plt.plot(epochs, loss, 'y', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Plotting training and validation dice score
dice = history.history['dice_metric']
val_dice = history.history['val_dice_metric']

plt.plot(epochs, dice, 'y', label='Training Dice Score')
plt.plot(epochs, val_dice, 'r', label='Validation Dice Score')
plt.title('Training and validation Dice Score')
plt.xlabel('Epochs')
plt.ylabel('Dice Score')
plt.legend()
plt.show()

# Plotting training and validation IOU score
IOU = history.history['iou_score']
val_IOU = history.history['val_iou_score']

plt.plot(epochs, IOU, 'y', label='Training IOU Score')
plt.plot(epochs, val_IOU, 'r', label='Validation IOU Score')
plt.title('Training and validation IOU Score')
plt.xlabel('Epochs')
plt.ylabel('IOU Score')
plt.legend()
plt.show()

import random

def return_test_images(images_path):
    # Get the list of image filenames from the specified directory
    image_filenames = [filename for filename in os.listdir(images_path)]
    # Randomly select 4 image filenames
    test_image_filenames = random.choices(image_filenames, k=4)

    # Load and preprocess the selected test images
    test_images = [Image.open(os.path.join(images_path, filename)).resize((224, 224)) for filename in test_image_filenames]
    processed_test_images = []

    # Process each test image
    for image in test_images:
        preprocessed_image = np.array(image)
        # Normalize the image if it meets the required shape
        if len(preprocessed_image.shape) == 3 and preprocessed_image.shape == (224, 224, 3):
            preprocessed_image = preprocessed_image / 255.0
        else:
            print("Invalid image shape:", preprocessed_image.shape)
            continue
        processed_test_images.append(preprocessed_image)

    processed_test_images = np.array(processed_test_images)

    print("Processed Test Images Shape:", processed_test_images.shape)
    return processed_test_images

# Path to the directory containing test images
test_images_path = '/kaggle/input/rm-segmentation-assignment-dataset/test-30'
test_images = return_test_images(test_images_path)

# Expand the dimensions to make it suitable for model prediction
image_input = np.expand_dims(test_images[0], 0)

# Get the prediction from the model
prediction = model.predict(image_input)[0]
# Convert prediction to a binary mask
predicted_mask = np.expand_dims(prediction.argmax(axis=-1), axis=-1)

# Print the shapes of the original image and the predicted mask
print("Original Image Shape:", test_images.shape)
print("Predicted Mask Shape:", predicted_mask.shape)

# Plotting
plt.figure(figsize=(12, 8))

# Plot the original image
plt.subplot(1, 2, 1)
plt.title('Original Image')
plt.imshow(test_images[0])
plt.axis('off')

# Plot the predicted mask
plt.subplot(1, 2, 2)
plt.title('Predicted Mask')
plt.imshow(predicted_mask[:, :, 0], cmap='gray')  # Plotting only the first channel of the mask
plt.axis('off')

plt.tight_layout()
plt.show()

for i, image in enumerate(test_images):
    # Expand the dimensions to make it suitable for model prediction
    image_input = np.expand_dims(image, 0)

    # Get the prediction from the model
    prediction = model.predict(image_input)[0]
    # Convert prediction to a binary mask
    predicted_mask = np.expand_dims(prediction.argmax(axis=-1), axis=-1)

    # Print the shapes of the original image and the predicted mask
    print("Original Image Shape:", image.shape)
    print("Predicted Mask Shape:", predicted_mask.shape)

    # Plotting
    plt.figure(figsize=(12, 8))

    # Plot the original image
    plt.subplot(1, 2, 1)
    plt.title('Original Image')
    plt.imshow(image)
    plt.axis('off')

    # Plot the predicted mask
    plt.subplot(1, 2, 2)
    plt.title('Predicted Mask')
    plt.imshow(predicted_mask)
    plt.axis('off')

    plt.tight_layout()
    plt.show()

