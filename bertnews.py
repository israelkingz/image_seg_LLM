# -*- coding: utf-8 -*-
"""bertNews.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FzYE184Q_Ar_sUQO0-uMOBglpvjJfnT7
"""

!pip install transformers datasets torch

#Preprocess the Dataset
#We need to tokenize our text data (news articles) before it can be fed to BERT. We also need to adjust the labels since BERT expects them to start from 0.
from datasets import load_dataset
from transformers import BertTokenizerFast

# Load the AG News dataset
dataset = load_dataset('ag_news')

# Load the BERT tokenizer
tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')

# Function to tokenize the text
def tokenize_function(examples):
    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128)

# Tokenize the dataset
tokenized_datasets = dataset.map(tokenize_function, batched=True)

# Example of accessing a tokenized example
print(tokenized_datasets['train'][0])

#Load the BERT Model for Sequence Classification

from transformers import BertForSequenceClassification

model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)

!pip install transformers[torch] accelerate -U

#Define Training Arguments and Train the Model
from transformers import BertForSequenceClassification, Trainer, TrainingArguments
import numpy as np

training_args = TrainingArguments(
    output_dir='./results',           # Output directory for model checkpoints
    num_train_epochs=3,               # Number of training epochs
    per_device_train_batch_size=8,    # Batch size for training
    per_device_eval_batch_size=8,     # Batch size for evaluation
    warmup_steps=500,                 # Number of warmup steps
    weight_decay=0.01,                # Weight decay rate
    logging_dir='./logs',             # Directory for logging
    evaluation_strategy='epoch',      # Evaluation strategy
)

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets['train'],
    eval_dataset=tokenized_datasets['test'],
    compute_metrics=lambda p: {"accuracy": (np.argmax(p.predictions, axis=1) == p.label_ids).mean()}
)

# Start fine-tuning
trainer.train()

eval_results = trainer.evaluate()
print(eval_results)