# -*- coding: utf-8 -*-
"""financial_phrasebank Dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mRbJ9dpwrsZQ9ftHNNYzgCDGs2Ipys6w
"""

!pip install datasets evaluate transformers[sentencepiece]
!pip install accelerate
# To run the training on TPU, you will need to uncomment the following line:
!pip install cloud-tpu-client==0.10 torch==1.9.0 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl
!apt install git-lfs

from datasets import load_dataset

# Load the Financial PhraseBank dataset
dataset = load_dataset("financial_phrasebank", "sentences_50agree")

dataset

import pandas as pd
df = pd.DataFrame(dataset['train'])

import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter

# 'df' is a pandas DataFrame that has already been created and contains the sentiment labels
labels = df['label']
label_counter = Counter(labels)

# Create a bar plot to show the distribution of sentiment labels
plt.figure(figsize=(7, 5))
sns.barplot(x=list(label_counter.keys()), y=list(label_counter.values()))
plt.title('Class Distribution')
plt.xlabel('Sentiment')
plt.ylabel('Number of Samples')
plt.show()

# Print the dataset structure and a sample
print(dataset)
print(dataset['train'][0:5])

#preprocessing
from transformers import AutoTokenizer

# Initialize the tokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# Function to tokenize the data
def tokenize_function(examples):
    return tokenizer(examples["sentence"], padding="max_length", truncation=True)

# Apply tokenization to all splits
tokenized_datasets = dataset.map(tokenize_function, batched=True)

#Split the data into train, validation, and test sets
from datasets import DatasetDict

# Split the original 'train' set into train, validation, and test sets
train_test_split = tokenized_datasets["train"].train_test_split(test_size=0.3)
# Further split the test set into validation and test
validation_test_split = train_test_split["test"].train_test_split(test_size=0.5)

# Combine splits into a new DatasetDict
split_datasets = DatasetDict({
    'train': train_test_split['train'],
    'validation': validation_test_split['train'],
    'test': validation_test_split['test']
})

#model setup
from transformers import AutoModelForSequenceClassification

# Load a pre-trained BERT model for sequence classification
model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=3)

#!pip install transformers[torch] accelerate -U

# Define Training Arguments
from transformers import Trainer, TrainingArguments

# # Define training parameters and configurations for the model
training_args = TrainingArguments(
    output_dir='./results',          # output directory for model checkpoints
    evaluation_strategy="epoch",     # evaluation is done at the end of each epoch
    learning_rate=2e-5,              # learning rate
    per_device_train_batch_size=8,   # batch size for training
    per_device_eval_batch_size=16,   # batch size for evaluation
    num_train_epochs=3,              # number of training epochs
    weight_decay=0.01                # strength of weight decay
)

#train the model
# Initialize the Trainer
import numpy as np
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=split_datasets['train'],
    eval_dataset=split_datasets['validation'],
    compute_metrics=lambda p: {"accuracy": (np.argmax(p.predictions, axis=1) == p.label_ids).mean()}
)

# Train the model
trainer.train()

# Evaluate the model on the test set
test_results = trainer.evaluate(split_datasets['test'])

# Print test results
print(test_results)

trainer.evaluate()

"""Overall Assessment:
Your model is showing a good balance between accuracy and efficiency, with a decent evaluation runtime and a high accuracy rate.
The loss value, while specific to the scale of your problem, suggests there may be room for improvement, either through further training, hyperparameter tuning, or by using a more complex model architecture.
Consider evaluating the model against a separate test set (if not already done) to ensure these metrics hold up in a completely unseen data scenario. This will help confirm the model's generalizability.
If your dataset is imbalanced or if there are multiple classes, consider looking at additional metrics such as precision, recall, F1-score, and a confusion matrix to get a more nuanced view of model performance across different categories.
"""

