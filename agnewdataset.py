# -*- coding: utf-8 -*-
"""AGnewdataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14J7qDnQppm7v3SCbhH6SHglchwhbc9lm
"""

#downloading neccessarily libraries
!pip install transformers[torch] accelerate -U
!pip install transformers datasets torch accelerate

#importing neccessarily libraries
import numpy as np
from datasets import load_dataset
from transformers import BertTokenizerFast, BertForSequenceClassification, Trainer, TrainingArguments

tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)

# Load the dataset and apply tokenization on a subset
def load_and_preprocess_data(subset_ratio=0.1):
    # Load dataset
    dataset = load_dataset('ag_news')
    tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')

    # Reduce dataset size by sampling a subset
    reduced_train_dataset = dataset['train'].shuffle(seed=42).select(range(int(dataset['train'].num_rows * subset_ratio)))
    reduced_test_dataset = dataset['test'].shuffle(seed=42).select(range(int(dataset['test'].num_rows * subset_ratio)))

    def tokenize_function(examples):
        # Tokenize the text to be suitable for BERT
        return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128)

    # Apply tokenization to all data splits
    tokenized_datasets = dataset.map(tokenize_function, batched=True)
    return tokenized_datasets

# Load the model
def load_model():
    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)
    return model

from transformers import Trainer, TrainingArguments

def train_model(tokenized_datasets, model):
    # Select a subset of the training and testing datasets
    # For instance, selecting the first 1000 examples from train and the first 500 from test
    train_subset = tokenized_datasets['train'].select(range(1000))
    test_subset = tokenized_datasets['test'].select(range(500))

    training_args = TrainingArguments(
        output_dir='./results',
        num_train_epochs=3,
        per_device_train_batch_size=8,
        per_device_eval_batch_size=8,
        warmup_steps=500,
        weight_decay=0.01,
        logging_dir='./logs',
        evaluation_strategy='epoch'
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_subset,
        eval_dataset=test_subset,
        compute_metrics=lambda p: {"accuracy": (np.argmax(p.predictions, axis=1) == p.label_ids).mean()}
    )

    trainer.train()
    return trainer

def save_model(model, tokenizer, save_path='AG_news_bertsentiment_model'):
    # Save the model and tokenizer
    model.save_pretrained(save_path)
    tokenizer.save_pretrained(save_path)

tokenized_datasets = load_and_preprocess_data()
model = load_model()
trainer = train_model(tokenized_datasets, model)
eval_results = trainer.evaluate()
print(eval_results)

save_model(model, tokenizer)  # Save the model and tokenizer

import torch

# Load the saved model and tokenizer
model_path = './AG_news_bertsentiment_model'
model = BertForSequenceClassification.from_pretrained(model_path)
tokenizer = BertTokenizerFast.from_pretrained(model_path)

# Prepare the model for inference (set to evaluation mode)
model.eval()

# Load the test dataset
dataset = load_dataset('ag_news', split='test')

# Function to prepare input data for the model
def prepare_data(texts, tokenizer):
    # Tokenize the text with the same parameters used in training
    encoding = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=128)
    return encoding

# Function to perform inference
def predict(model, inputs):
    with torch.no_grad():
        outputs = model(**inputs)
        predictions = torch.argmax(outputs.logits, dim=1)
    return predictions

# Sample a few examples from the test dataset
sample_data = dataset.select(range(10))  # Select the first 10 examples for demonstration

# Extract texts from the sampled data
texts = sample_data['text']

# Prepare data
inputs = prepare_data(texts, tokenizer)

# Perform inference
predictions = predict(model, inputs)

# Mapping of AG News labels
label_map = {0: "World", 1: "Sports", 2: "Business", 3: "Sci/Tech"}

# Print predictions
print("Inference Results:")
for text, label, pred in zip(texts, sample_data['label'], predictions):
    print(f"Sentence: '{text}'")
    print(f"True Label: {label_map[label]} - Predicted Category: {label_map[pred.item()]}")
    print()

